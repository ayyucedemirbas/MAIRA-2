{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN8lMAeuqxwD+bkI26YZw0W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayyucedemirbas/MAIRA-2/blob/main/MAIRA-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WzZO3NTGL44"
      },
      "outputs": [],
      "source": [
        "!pip install protobuf sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git@88d960937c81a32bfb63356a2e8ecf7999619681"
      ],
      "metadata": {
        "id": "rb9XFUywGSH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "zY4QZaIeG-ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "eFR4K9wGHIOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "import requests\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor"
      ],
      "metadata": {
        "id": "As71IPWGH43U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_model_cache = {}\n",
        "\n",
        "def load_model_and_processor(hf_token: str):\n",
        "    \"\"\"\n",
        "    Loads the MAIRA-2 model and processor from Hugging Face using the provided token.\n",
        "    The loaded objects are cached keyed by the token.\n",
        "    \"\"\"\n",
        "    if hf_token in _model_cache:\n",
        "        return _model_cache[hf_token]\n",
        "    device = torch.device(\"cpu\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"microsoft/maira-2\",\n",
        "        trust_remote_code=True,\n",
        "        use_auth_token=hf_token\n",
        "    )\n",
        "    processor = AutoProcessor.from_pretrained(\n",
        "        \"microsoft/maira-2\",\n",
        "        trust_remote_code=True,\n",
        "        use_auth_token=hf_token\n",
        "    )\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    _model_cache[hf_token] = (model, processor)\n",
        "    return model, processor"
      ],
      "metadata": {
        "id": "OFphjSsAH_rK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sample_data() -> dict:\n",
        "    \"\"\"\n",
        "    Downloads sample chest X-ray images and associated data.\n",
        "    \"\"\"\n",
        "    frontal_image_url = \"https://openi.nlm.nih.gov/imgs/512/145/145/CXR145_IM-0290-1001.png\"\n",
        "    lateral_image_url = \"https://openi.nlm.nih.gov/imgs/512/145/145/CXR145_IM-0290-2001.png\"\n",
        "\n",
        "    def download_and_open(url: str) -> Image.Image:\n",
        "        response = requests.get(url, headers={\"User-Agent\": \"MAIRA-2\"}, stream=True)\n",
        "        return Image.open(response.raw).convert(\"RGB\")\n",
        "\n",
        "    frontal = download_and_open(frontal_image_url)\n",
        "    lateral = download_and_open(lateral_image_url)\n",
        "    return {\n",
        "        \"frontal\": frontal,\n",
        "        \"lateral\": lateral,\n",
        "        \"indication\": \"Dyspnea.\",\n",
        "        \"technique\": \"PA and lateral views of the chest.\",\n",
        "        \"comparison\": \"None.\",\n",
        "        \"phrase\": \"Pleural effusion.\"\n",
        "    }"
      ],
      "metadata": {
        "id": "w8b5b3i_ICnR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_report(hf_token, frontal, lateral, indication, technique, comparison, use_grounding):\n",
        "    \"\"\"\n",
        "    Generates a radiology report using the MAIRA-2 model.\n",
        "    If any image/text input is missing, sample data is used.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model, processor = load_model_and_processor(hf_token)\n",
        "    except Exception as e:\n",
        "        return f\"Error loading model: {str(e)}\"\n",
        "    device = torch.device(\"cpu\")\n",
        "    sample = get_sample_data()\n",
        "    if frontal is None:\n",
        "        frontal = sample[\"frontal\"]\n",
        "    if lateral is None:\n",
        "        lateral = sample[\"lateral\"]\n",
        "    if not indication:\n",
        "        indication = sample[\"indication\"]\n",
        "    if not technique:\n",
        "        technique = sample[\"technique\"]\n",
        "    if not comparison:\n",
        "        comparison = sample[\"comparison\"]\n",
        "\n",
        "    processed_inputs = processor.format_and_preprocess_reporting_input(\n",
        "        current_frontal=frontal,\n",
        "        current_lateral=lateral,\n",
        "        prior_frontal=None,  # No prior study is used in this demo.\n",
        "        indication=indication,\n",
        "        technique=technique,\n",
        "        comparison=comparison,\n",
        "        prior_report=None,\n",
        "        return_tensors=\"pt\",\n",
        "        get_grounding=use_grounding,\n",
        "    )\n",
        "    # Move all tensors to the CPU\n",
        "    processed_inputs = {k: v.to(device) for k, v in processed_inputs.items()}\n",
        "    # Remove keys containing \"image_sizes\" to prevent unexpected keyword errors.\n",
        "    processed_inputs = dict(processed_inputs)\n",
        "    keys_to_remove = [k for k in processed_inputs if \"image_sizes\" in k]\n",
        "    for key in keys_to_remove:\n",
        "        processed_inputs.pop(key, None)\n",
        "\n",
        "    max_tokens = 450 if use_grounding else 300\n",
        "    with torch.no_grad():\n",
        "        output_decoding = model.generate(\n",
        "            **processed_inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            use_cache=True,\n",
        "        )\n",
        "    prompt_length = processed_inputs[\"input_ids\"].shape[-1]\n",
        "    decoded_text = processor.decode(output_decoding[0][prompt_length:], skip_special_tokens=True)\n",
        "    decoded_text = decoded_text.lstrip()  # Remove any leading whitespace\n",
        "    prediction = processor.convert_output_to_plaintext_or_grounded_sequence(decoded_text)\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "VGx8_gDIIJ3S"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_phrase_grounding(hf_token, frontal, phrase):\n",
        "    \"\"\"\n",
        "    Runs phrase grounding using the MAIRA-2 model.\n",
        "    If image or phrase is missing, sample data is used.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model, processor = load_model_and_processor(hf_token)\n",
        "    except Exception as e:\n",
        "        return f\"Error loading model: {str(e)}\"\n",
        "    device = torch.device(\"cpu\")\n",
        "    sample = get_sample_data()\n",
        "    if frontal is None:\n",
        "        frontal = sample[\"frontal\"]\n",
        "    if not phrase:\n",
        "        phrase = sample[\"phrase\"]\n",
        "    processed_inputs = processor.format_and_preprocess_phrase_grounding_input(\n",
        "        frontal_image=frontal,\n",
        "        phrase=phrase,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    processed_inputs = {k: v.to(device) for k, v in processed_inputs.items()}\n",
        "    # Remove keys containing \"image_sizes\" to prevent unexpected keyword errors.\n",
        "    processed_inputs = dict(processed_inputs)\n",
        "    keys_to_remove = [k for k in processed_inputs if \"image_sizes\" in k]\n",
        "    for key in keys_to_remove:\n",
        "        processed_inputs.pop(key, None)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_decoding = model.generate(\n",
        "            **processed_inputs,\n",
        "            max_new_tokens=150,\n",
        "            use_cache=True,\n",
        "        )\n",
        "    prompt_length = processed_inputs[\"input_ids\"].shape[-1]\n",
        "    decoded_text = processor.decode(output_decoding[0][prompt_length:], skip_special_tokens=True)\n",
        "    prediction = processor.convert_output_to_plaintext_or_grounded_sequence(decoded_text)\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "K-5rHijyIN76"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def login_ui(hf_token):\n",
        "    \"\"\"Authenticate the user by loading the model.\"\"\"\n",
        "    try:\n",
        "        load_model_and_processor(hf_token)\n",
        "        return \"ðŸ”“ Login successful! You can now use the model.\"\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Login failed: {str(e)}\""
      ],
      "metadata": {
        "id": "q6w3rAeDIQMB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_report_ui(hf_token, frontal_path, lateral_path, indication, technique, comparison,\n",
        "                         prior_frontal_path, prior_lateral_path, prior_report, grounding):\n",
        "    \"\"\"\n",
        "    Wrapper for generate_report that accepts file paths (from the UI) for images.\n",
        "    Prior study fields are ignored.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        frontal = Image.open(frontal_path) if frontal_path else None\n",
        "        lateral = Image.open(lateral_path) if lateral_path else None\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error loading images: {str(e)}\"\n",
        "    return generate_report(hf_token, frontal, lateral, indication, technique, comparison, grounding)"
      ],
      "metadata": {
        "id": "2H6su9ChISao"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_phrase_grounding_ui(hf_token, frontal_path, phrase):\n",
        "    \"\"\"\n",
        "    Wrapper for run_phrase_grounding that accepts a file path for the frontal image.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        frontal = Image.open(frontal_path) if frontal_path else None\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error loading image: {str(e)}\"\n",
        "    return run_phrase_grounding(hf_token, frontal, phrase)"
      ],
      "metadata": {
        "id": "K6-6YvdhIUzw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_temp_image(img: Image.Image) -> str:\n",
        "    \"\"\"Save a PIL image to a temporary file and return the file path.\"\"\"\n",
        "    temp_file = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n",
        "    img.save(temp_file.name)\n",
        "    return temp_file.name"
      ],
      "metadata": {
        "id": "Ior0_cuuIXCl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sample_findings():\n",
        "    \"\"\"\n",
        "    Loads sample data for the report generation tab.\n",
        "    Returns file paths for current study images, sample text fields, and dummy values for prior study.\n",
        "    \"\"\"\n",
        "    sample = get_sample_data()\n",
        "    return [\n",
        "        save_temp_image(sample[\"frontal\"]),  # frontal image file path\n",
        "        save_temp_image(sample[\"lateral\"]),    # lateral image file path\n",
        "        sample[\"indication\"],\n",
        "        sample[\"technique\"],\n",
        "        sample[\"comparison\"],\n",
        "        None,  # prior frontal (not used)\n",
        "        None,  # prior lateral (not used)\n",
        "        None,  # prior report (not used)\n",
        "        False  # grounding checkbox default\n",
        "    ]"
      ],
      "metadata": {
        "id": "nrSVoRy3IaL3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sample_phrase():\n",
        "    \"\"\"\n",
        "    Loads sample data for the phrase grounding tab.\n",
        "    Returns file path for the frontal image and a sample phrase.\n",
        "    \"\"\"\n",
        "    sample = get_sample_data()\n",
        "    return [save_temp_image(sample[\"frontal\"]), sample[\"phrase\"]]"
      ],
      "metadata": {
        "id": "ntzXAmeSIcQE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(title=\"MAIRA-2 Medical Assistant\") as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # MAIRA-2 Medical Assistant\n",
        "        **Authentication required** - You need a Hugging Face account and access token to use this model.\n",
        "        1. Get your access token from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "        2. Request model access at [https://huggingface.co/microsoft/maira-2](https://huggingface.co/microsoft/maira-2)\n",
        "        3. Paste your token below to begin\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        hf_token = gr.Textbox(\n",
        "            label=\"Hugging Face Token\",\n",
        "            placeholder=\"hf_xxxxxxxxxxxxxxxxxxxx\",\n",
        "            type=\"password\"\n",
        "        )\n",
        "        login_btn = gr.Button(\"Authenticate\")\n",
        "        login_status = gr.Textbox(label=\"Authentication Status\", interactive=False)\n",
        "\n",
        "    login_btn.click(\n",
        "        login_ui,\n",
        "        inputs=hf_token,\n",
        "        outputs=login_status\n",
        "    )\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.Tab(\"Report Generation\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"## Current Study\")\n",
        "                    frontal = gr.Image(label=\"Frontal View\", type=\"filepath\")\n",
        "                    lateral = gr.Image(label=\"Lateral View\", type=\"filepath\")\n",
        "                    indication = gr.Textbox(label=\"Clinical Indication\")\n",
        "                    technique = gr.Textbox(label=\"Imaging Technique\")\n",
        "                    comparison = gr.Textbox(label=\"Comparison\")\n",
        "\n",
        "                    gr.Markdown(\"## Prior Study (Optional)\")\n",
        "                    prior_frontal = gr.Image(label=\"Prior Frontal View\", type=\"filepath\")\n",
        "                    prior_lateral = gr.Image(label=\"Prior Lateral View\", type=\"filepath\")\n",
        "                    prior_report = gr.Textbox(label=\"Prior Report\")\n",
        "\n",
        "                    grounding = gr.Checkbox(label=\"Include Grounding\")\n",
        "                    sample_btn = gr.Button(\"Load Sample Data\")\n",
        "                with gr.Column():\n",
        "                    report_output = gr.Textbox(label=\"Generated Report\", lines=10)\n",
        "                    generate_btn = gr.Button(\"Generate Report\")\n",
        "\n",
        "            sample_btn.click(\n",
        "                load_sample_findings,\n",
        "                outputs=[frontal, lateral, indication, technique, comparison,\n",
        "                         prior_frontal, prior_lateral, prior_report, grounding]\n",
        "            )\n",
        "            generate_btn.click(\n",
        "                generate_report_ui,\n",
        "                inputs=[hf_token, frontal, lateral, indication, technique, comparison,\n",
        "                        prior_frontal, prior_lateral, prior_report, grounding],\n",
        "                outputs=report_output\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"Phrase Grounding\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    pg_frontal = gr.Image(label=\"Frontal View\", type=\"filepath\")\n",
        "                    phrase = gr.Textbox(label=\"Phrase to Ground\")\n",
        "                    pg_sample_btn = gr.Button(\"Load Sample Data\")\n",
        "                with gr.Column():\n",
        "                    pg_output = gr.Textbox(label=\"Grounding Result\", lines=3)\n",
        "                    pg_btn = gr.Button(\"Find Phrase\")\n",
        "\n",
        "            pg_sample_btn.click(\n",
        "                load_sample_phrase,\n",
        "                outputs=[pg_frontal, phrase]\n",
        "            )\n",
        "            pg_btn.click(\n",
        "                run_phrase_grounding_ui,\n",
        "                inputs=[hf_token, pg_frontal, phrase],\n",
        "                outputs=pg_output\n",
        "            )"
      ],
      "metadata": {
        "id": "M3PonGysHdXy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "41rWR2H5IhmR",
        "outputId": "2f9b1fcf-a2d4-4df3-b01b-1b21898b333c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
            "----\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7375a0b33087afcb70.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7375a0b33087afcb70.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}